unofficial wikipedia dump with 5 million+ english articles with parsed/cleaned texts

format: bzip2 compressed text file, each line contains json object with url/title/body fields representing separate article

dump version/date: unknown  
wikitext parser: unknown  

downloaded from: https://www.dropbox.com/s/wwnfnu441w1ec9p/wiki-articles.json.bz2

originally mentioned in [tantivy-cli tutorial](https://github.com/quickwit-oss/tantivy-cli/blob/3431be4fca7c1ac106d2e0172cde199fa877ad2d/README.md?plain=1#L156) as dataset for testing/benchmarking tantivy indexing
